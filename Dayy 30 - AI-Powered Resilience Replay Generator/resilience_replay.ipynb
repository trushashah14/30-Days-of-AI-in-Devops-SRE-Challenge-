{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57352f1",
   "metadata": {},
   "source": [
    "# Day 30: AI-Powered Resilience Replay Generator\n",
    "\n",
    "Reconstruct incident timelines, simulate expected system behavior, and generate replay narratives using ML and LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a166f031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## üß† Resilience Replay Narrative\n",
       "**Replay Narrative**\n",
       "\n",
       "On August 1st, 2025, at 14:00:00, our monitoring systems detected an incident affecting multiple services.\n",
       "\n",
       "**14:00:00 - Service B**\n",
       "\n",
       "Expected: Autoscale to 6 replicas to handle increased traffic.\n",
       "Actual: Stalled at 2 replicas, resulting in a deviation of -4.0.\n",
       "\n",
       "Impact: Service C started experiencing delays due to the bottleneck in Service B.\n",
       "\n",
       "**14:03:00 - Service B (continued)**\n",
       "\n",
       "Expected: Autoscale to 6 replicas to maintain performance.\n",
       "Actual: Still stalled at 2 replicas, resulting in a deviation of -4.0 for the second consecutive minute.\n",
       "\n",
       "Impact: Latency continued to increase in Service C, causing user-facing issues.\n",
       "\n",
       "**14:05:00 - Service C**\n",
       "\n",
       "Expected: Latency should remain within the threshold (120ms).\n",
       "Actual: Latency surged to 300ms, with a deviation of 180.0.\n",
       "\n",
       "Impact: Users experienced significant delays when interacting with Service C, potentially leading to frustration and abandonment.\n",
       "\n",
       "In summary, an unexpected issue prevented Service B from autoscaling, causing a cascade effect on related services like Service C. We are investigating the root cause and implementing corrective actions to ensure prompt recovery and prevent future incidents."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üì¶ Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# üìÅ Load data\n",
    "incident = pd.read_csv('incident_metadata.csv', parse_dates=['timestamp'])\n",
    "baseline = pd.read_csv('historical_baseline.csv', parse_dates=['timestamp'])\n",
    "actual = pd.read_csv('actual_metrics.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# üß† Simulate expected behavior using ML (simple regression per metric/service)\n",
    "simulated = []\n",
    "for idx, row in incident.iterrows():\n",
    "    service = row['service']\n",
    "    ts = row['timestamp']\n",
    "    expected_sla = row['expected_sla']\n",
    "    # Find baseline metric for this service\n",
    "    base_rows = baseline[(baseline['service'] == service)]\n",
    "    actual_rows = actual[(actual['service'] == service) & (actual['timestamp'] == ts)]\n",
    "    for metric in base_rows['metric'].unique():\n",
    "        base_metric = base_rows[base_rows['metric'] == metric]\n",
    "        if len(base_metric) < 2:\n",
    "            continue\n",
    "        X = np.arange(len(base_metric)).reshape(-1, 1)\n",
    "        y = base_metric['value'].values\n",
    "        model = LinearRegression().fit(X, y)\n",
    "        pred = model.predict(np.array([[len(base_metric)]]))[0]\n",
    "        actual_val = actual_rows[actual_rows['metric'] == metric]['value'].values\n",
    "        actual_val = actual_val[0] if len(actual_val) > 0 else None\n",
    "        deviation = None if actual_val is None else actual_val - pred\n",
    "        simulated.append({\n",
    "            'timestamp': ts,\n",
    "            'service': service,\n",
    "            'metric': metric,\n",
    "            'expected': pred,\n",
    "            'actual': actual_val,\n",
    "            'deviation': deviation\n",
    "        })\n",
    "\n",
    "# üìä Pinpoint deviation zones\n",
    "deviation_zones = [s for s in simulated if s['deviation'] is not None and abs(s['deviation']) > 1]\n",
    "\n",
    "# üß† Generate replay narrative using LLM (Ollama Llama 3)\n",
    "def llm_replay_narrative(deviation_zones):\n",
    "    prompt = (\n",
    "        \"You are an SRE assistant. Given the following incident deviation zones, generate a replay narrative that:\\n\"\n",
    "        \"- For each timestamp, describe what should have happened (expected) and what actually occurred (actual).\\n\"\n",
    "        \"- Highlight key deviations and their impact on related services.\\n\"\n",
    "        \"- Use concise, stakeholder-friendly language.\\n\"\n",
    "        \"- Example: 'At 14:03, Service B should have autoscaled to 6 pods. Instead, it stalled at 2, causing cascading latency in Service C.'\\n\\n\"\n",
    "    )\n",
    "    for s in deviation_zones:\n",
    "        prompt += (\n",
    "            f\"Timestamp: {s['timestamp']}, Service: {s['service']}, Metric: {s['metric']}, Expected: {s['expected']}, Actual: {s['actual']}, Deviation: {s['deviation']}\\n\"\n",
    "        )\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\"model\": \"llama3\", \"prompt\": prompt, \"stream\": False}\n",
    "    )\n",
    "    return response.json().get(\"response\", \"No response from Llama 3.\")\n",
    "\n",
    "# üìä Display replay narrative\n",
    "narrative = llm_replay_narrative(deviation_zones)\n",
    "display(Markdown(\"## üß† Resilience Replay Narrative\\n\" + narrative))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
