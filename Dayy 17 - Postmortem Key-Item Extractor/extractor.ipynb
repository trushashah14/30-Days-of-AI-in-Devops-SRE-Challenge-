{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c295a967",
   "metadata": {},
   "source": [
    "# Postmortem Key-Item Extractor \n",
    "\n",
    "This notebook extracts root causes, action items, and owners from incident reports using Llama 3 via Ollama.\n",
    "\n",
    "Make sure Ollama is running locally with the Llama 3 model pulled (`ollama pull llama3`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d5c811",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n",
    "\n",
    "Install Python libraries for data handling and LLM integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acec023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers accelerate --quiet\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3a5194",
   "metadata": {},
   "source": [
    "## Step 2: Load Incident Data\n",
    "\n",
    "Read the incident report(s) from `raw_data.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18e68d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['affected_activity',\n",
       " 'assignment_group',\n",
       " 'assigned_to',\n",
       " 'assistant_groups',\n",
       " 'cie_assigned_to',\n",
       " 'audience',\n",
       " 'caller',\n",
       " 'close_code',\n",
       " 'close_notes',\n",
       " 'contact_type',\n",
       " 'crn_masks',\n",
       " 'current_status_next_steps',\n",
       " 'customer_facing_impact',\n",
       " 'customers_impacted',\n",
       " 'child_incidents',\n",
       " 'detection_source',\n",
       " 'long_description',\n",
       " 'outage_start',\n",
       " 'outage_end',\n",
       " 'recurring_event',\n",
       " 'severity',\n",
       " 'short_description',\n",
       " 'technical_bridge_start',\n",
       " 'tribe',\n",
       " 'problem',\n",
       " 'caused_by_change_number',\n",
       " 'caused_by_change',\n",
       " 'detection_time',\n",
       " 'identification_time',\n",
       " 'mitigation_time',\n",
       " 'was_customer_impacted',\n",
       " 'impact_adjustment_factor',\n",
       " 'unit_type',\n",
       " 'units_affected',\n",
       " 'total_units',\n",
       " 'chronology',\n",
       " 'disable_pager',\n",
       " 'status',\n",
       " 'state',\n",
       " 'created',\n",
       " 'updated',\n",
       " 'href',\n",
       " 'number',\n",
       " 'linked_cases',\n",
       " 'sys_id',\n",
       " 'resolved',\n",
       " 'resolved_by',\n",
       " 'affected_ci_list',\n",
       " 'created_by',\n",
       " 'disruption_time',\n",
       " 'comment_list']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"raw_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    incident_data = json.load(f)\n",
    "\n",
    "# Optional: preview keys\n",
    "list(incident_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a2ef07",
   "metadata": {},
   "source": [
    "## Step 3: Format the LLM Prompt\n",
    "\n",
    "Create a prompt for Llama 3 to extract root cause, action items, and owners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2514e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(incident: dict) -> str:\n",
    "    context = json.dumps(incident, indent=2)\n",
    "    prompt = f\"\"\"\n",
    "You are a postmortem analyst. Extract the following from this incident report:\n",
    "- Root Cause\n",
    "- Action Items\n",
    "- Responsible Owners\n",
    "\n",
    "Respond in JSON format with keys: root_cause, action_items, owners.\n",
    "\n",
    "Incident Report:\n",
    "{context}\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce01fb94",
   "metadata": {},
   "source": [
    "## Step 4: Connect to Ollama and Generate Output\n",
    "\n",
    "Send the prompt to Llama 3 via Ollama and collect the streamed response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c10854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "def check_ollama():\n",
    "    try:\n",
    "        r = requests.get(\"http://localhost:11434\")\n",
    "        return r.status_code == 200\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def get_llm_output(prompt):\n",
    "    ollama_url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"llama3\",  # Use smaller model for faster response\n",
    "        \"prompt\": prompt,\n",
    "        \"options\": {\"num_predict\": 512}\n",
    "    }\n",
    "    llm_output = \"\"\n",
    "    try:\n",
    "        with requests.post(ollama_url, json=payload, stream=True) as response:\n",
    "            response.raise_for_status()\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    try:\n",
    "                        part = json.loads(line.decode())\n",
    "                        llm_output += part.get(\"response\", \"\")\n",
    "                    except Exception:\n",
    "                        pass\n",
    "    except Exception as e:\n",
    "        print(\"Ollama connection or generation error:\", e)\n",
    "    return llm_output\n",
    "\n",
    "if not check_ollama():\n",
    "    print(\"Ollama server is not running. Please start Ollama and pull the llama3 model.\")\n",
    "else:\n",
    "    prompt = format_prompt(incident_data)\n",
    "    llm_output = get_llm_output(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36329c27",
   "metadata": {},
   "source": [
    "## Step 5: Extract and Display JSON Output\n",
    "\n",
    "Parse the LLM output to extract the structured JSON and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f90e33a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action_items': ['Cleaned up Fabcon DB in the SDN dev team in the dev '\n",
      "                  'environment',\n",
      "                  'Transferred cleaned-up DBs to respective smartnics along '\n",
      "                  'with a fabcon snap restart'],\n",
      " 'owners': ['@nikolay.parvov@bg.ibm.com',\n",
      "            '@amul.sharma1@ibm.com',\n",
      "            '@Lekshmanan S',\n",
      "            '@Gopi.selvaraj',\n",
      "            '@Aniruddh Prakash',\n",
      "            '@John Haughton'],\n",
      " 'root_cause': 'Corrupted security group ID within the fabcon DB structure '\n",
      "               'preventing fabcon server from starting up correctly on DPUs on '\n",
      "               'node mon3-qz1-sr3-rk168-s18.'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>root_cause</th>\n",
       "      <th>action_items</th>\n",
       "      <th>owners</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Corrupted security group ID within the fabcon ...</td>\n",
       "      <td>[Cleaned up Fabcon DB in the SDN dev team in t...</td>\n",
       "      <td>[@nikolay.parvov@bg.ibm.com, @amul.sharma1@ibm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          root_cause  \\\n",
       "0  Corrupted security group ID within the fabcon ...   \n",
       "\n",
       "                                        action_items  \\\n",
       "0  [Cleaned up Fabcon DB in the SDN dev team in t...   \n",
       "\n",
       "                                              owners  \n",
       "0  [@nikolay.parvov@bg.ibm.com, @amul.sharma1@ibm...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Improved JSON extraction and validation\n",
    "def extract_json(text):\n",
    "    start = text.find(\"{\")\n",
    "    end = text.rfind(\"}\") + 1\n",
    "    if start == -1 or end == -1:\n",
    "        print(\"No JSON found in LLM output.\")\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(text[start:end])\n",
    "    except Exception as e:\n",
    "        print(\"Error extracting JSON:\", e)\n",
    "        return {}\n",
    "\n",
    "extracted_json = extract_json(llm_output)\n",
    "\n",
    "# Display extracted JSON\n",
    "import pprint\n",
    "pprint.pprint(extracted_json)\n",
    "\n",
    "# Display as DataFrame\n",
    "df = pd.DataFrame([extracted_json])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd9d806",
   "metadata": {},
   "source": [
    "## Step 6: Save Results\n",
    "\n",
    "Save the extracted key items to CSV and JSON files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7ca4217",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"llama3_extracted_postmortem.csv\", index=False)\n",
    "\n",
    "with open(\"llama3_extracted_postmortem.json\", \"w\") as f:\n",
    "    json.dump(extracted_json, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
