# Day 25: Performance Regression Detector üö¶ ‚Äì Sept 1, 2025

## Objective üöÄ
- Compare benchmark runs between versions/releases
- Statistically identify significant performance regressions
- Flag and report regressions for SRE/DevOps teams

## Code & Implementation üíª
- **Script**: `performance_regression.py`  
  Main workflow for loading benchmark data, ML-based regression detection, and reporting.
- **Notebook**: [`performance_regression.ipynb`](./performance_regression.ipynb)  
  Interactive workflow for data loading, ML/AI regression detection, visualization, and interpretation.
- **Step-by-Step Solution**: [`Step-by-Step-Solution.md`](./Step-by-Step-Solution.md)  
  Detailed guide for setup, workflow, and interpretation.
- **Sample Data**:  
  - `old_benchmark.csv`, `new_benchmark.csv` ‚Äì benchmark results from previous and current versions
- **Output**:  
  - Summary table with metrics, means, ML-predicted regression flags

## How Are Benchmark Files Created in the Real World? üìä
Benchmark files are typically generated by running automated performance tests or benchmarks on your application or system. These tests measure metrics like latency, throughput, error rate, CPU/memory usage, etc.

**Common sources for benchmark files:**
- CI/CD Pipelines: Performance tests run automatically on each commit, PR, or release. Results are saved as CSV/JSON files.
- Benchmarking Tools: Tools like `pytest-benchmark`, `locust`, `JMeter`, `wrk`, or custom scripts output results to files.
- Monitoring Systems: Metrics exported from Prometheus, Grafana, Datadog, etc., can be saved as CSV for analysis.
- Manual Runs: Engineers run scripts or tools locally and save results.

**Typical workflow:**
1. Run performance tests on old and new versions of your code.
2. Export results to CSV/JSON files.
3. Feed these files into the regression detector (script or notebook).

**Example:**
- After a CI build, a script runs `pytest-benchmark` and saves results as `benchmark_old.csv`.
- After a new commit, the same script runs again and saves `benchmark_new.csv`.
- These files are then compared for regressions.

## Workflow üîÑ
1. **Collect Benchmark Data:**  
   Gather performance metrics (e.g., latency, throughput) from multiple versions.
2. **Load Data:**  
   Use pandas to load CSV files into DataFrames.
3. **Compare Results:**  
   Use statistical tests (t-test) or ML classifiers to compare metrics.
4. **Flag Regressions:**  
   Automatically detect and report significant regressions.
5. **Report:**  
   Output results in a clear format (console, markdown, or dashboard).

## Why Each Step Was Chosen üìä
- **Statistical Testing:**  
  Provides objective, explainable regression detection.
- **Automation:**  
  Enables integration into CI/CD pipelines for continuous reliability.
- **Reporting:**  
  Makes findings actionable for SRE/DevOps teams.

## Usage

```bash
python performance_regression.py old_benchmark.csv new_benchmark.csv
```

Or use the notebook for interactive analysis.

## Requirements

- Python 3.8+
- `scipy`, `pandas`, `sklearn` (for ML classifier option)

## Interpretation of Results üß†
- **Statistical Test Output:**  
  p-value and significance for each metric.
- **Regression Flag:**  
  Highlights metrics with significant performance drops.

## How to Use in Real-World DevOps/SRE üåç

### Automated Regression Detection
**Use Case:**  
Catch performance regressions before they reach production.

**Implementation:**  
- Integrate script into CI/CD pipelines.
- Compare benchmark files from previous and current builds.
- Flag and report regressions for review.

**Industry Examples:**  
- Release Validation: Ensure new releases do not degrade performance.
- Incident Prevention: Detect regressions before they impact users.

## Where Was AI Used? ü§ñ

- **AI Usage:**  
    
  In this project, scikit-learn is used to train and apply ML models (such as logistic regression or decision trees) on historical benchmark metrics to identify complex regression patterns.

**AI Technologies Utilized:**  
- Python (for statistical analysis and ML workflows)
- Scikit-learn (ML classifiers for regression detection)
- Pandas (data manipulation and preprocessing)
- Scipy (statistical testing)

## References üìñ
- [Scipy Statistical Tests](https://docs.scipy.org/doc/scipy/reference/stats.html)
- [Benchmarking Best Practices](https://sre.google/sre-book/performance-and-reliability/)
- [CI/CD Integration](https://martinfowler.com/articles/continuousIntegration.html)

## Future Enhancements üöÄ
- Integrate with CI/CD for automated regression checks
- Support more benchmark formats and metrics
- Add dashboard visualization for trends and regressions

